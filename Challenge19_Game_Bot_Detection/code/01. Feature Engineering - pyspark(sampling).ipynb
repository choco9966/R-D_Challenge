{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm_notebook # check the progressbar in the python. \n",
    "import glob # check the file name in fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "path = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_lst = glob.glob(path + '*')\n",
    "read_file_lst[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "for i in tqdm_notebook(read_file_lst):\n",
    "    # sqlContext.read.csv() : path를 C:Users/..의 형식으로 맞춰줘야함. (초기값이 spark설치된 곳으로 지정되어있음)\n",
    "    df_temp = sqlContext.read.csv(SparkFiles.get(i), header=True, inferSchema= True) # read.csv \n",
    "    \n",
    "    # transformation으로 추가하려면, df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "    df_temp = df_temp.withColumn(\"Date\", lit(i[-8:-4])) # column 추가, lit명령어가 value를 추가하는 방법. \n",
    "    \n",
    "    if i == read_file_lst[0]:\n",
    "        df_total = df_temp\n",
    "    else:\n",
    "        df_total = unionAll([df_total, df_temp]) # row-wise 결합\n",
    "        \n",
    "    del df_temp \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# userItem=df.groupby('userId').agg(f.expr('count(distinct item)').alias('n_item'))\n",
    "# df_total.groupBy(\"log_id\").agg(f.expr('count(distinct item)').alias('log_id'))\n",
    "# df_total.select(\"actor_account\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/'\n",
    "train_label = sqlContext.read.csv(SparkFiles.get(i + 'labeled_accounts.csv'), header=True, inferSchema= True)\n",
    "test_label = sqlContext.read.csv(SparkFiles.get(i + 'test_accounts.csv'), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_label.count(), len(train_label.columns)))\n",
    "print((test_label.count(), len(test_label.columns)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- train의 size : (356997380, 28)\n",
    "- test의 size : (362572164, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player information features\n",
    "- Actor (o)\n",
    "- A_Acc (o)\n",
    "- login_day_count \n",
    "- logout_day_count\n",
    "- playtime (o)\n",
    "- playtime_per_day\n",
    "- avg_money (o)\n",
    "- login_count (o)\n",
    "- ip_count (o)\n",
    "- max_level (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드의 개발을 위해서 데이터를 먼저 0.01%만 샘플링해서 사용. \n",
    "# df_total_sample = df_total.rdd.takeSample(False, 0.0001, seed = 1234)\n",
    "\n",
    "# 하지만, 샘플링의 경우 모든 row를 훑기때문에 속도가 느리고 \n",
    "# 내가 원한거는 디버깅용이므로 상위 10만개의 row만 추출하도록 수정 \n",
    "# 방법1. index column 이 있으면 Between 사용 : \n",
    "# 예) df_total.where(col(\"actor\").between(886535, 8865350))\n",
    "df_total_sample = df_total.where(df_total.actor.between(800000, 800100))\n",
    "\n",
    "# 방법2. index column 을 추가해서 자르는 방법이 있음. \n",
    "# 예) from pyspark.sql.functions import monotonically_increasing_id\n",
    "# df.withColumn(\"id\", monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_total_sample.count(), len(df_total_sample.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : login_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func # pyspark의 유용한 기능을 사용하는 패키지 : countDistinct 사용가능\n",
    "# 원하는 부분만 출력할때는 df.filter()를 사용. \n",
    "\n",
    "# 아이온데이터에 actor는 다르지만 actor_account가 같은 경우는 있는 것 같음. \n",
    "# 하지만 제출형태가 actor_account를 기준으로 해가지고 groupby를 아래의 변수로 진행했음. \n",
    "df_total_sample = df_total_sample.withColumnRenamed(\"actor_account\", \"account\")\n",
    "# 컬럼명을 변경하는 방법으로는 .select(col(\"count\").alias(\"login_count\"))이라는 방법을 사용합니다. \n",
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 103).groupBy('account').agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"login_count\")\n",
    "# pyspark에서 join의 경우 .join(how='left', table1.id == table2.id)의 문법을 사용함. \n",
    "train_label = train_label.join(df_total_sample_agg, ['account'] , how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'] , how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : Day_unique_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sample_agg = df_total_sample.groupby(df_total_sample.account).agg(func.countDistinct('Date')).withColumnRenamed(\"count(DISTINCT Date)\", \"Day_unique_count\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : ip_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 103).groupBy('account').agg(func.countDistinct('etc_str1')).withColumnRenamed(\"count(DISTINCT etc_str1)\", \"ip_count\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : max_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 160).groupBy('account').agg({'etc_num2':'max'}).withColumnRenamed(\"max(etc_num2)\", \"max_level\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 104).groupBy('account').agg({'etc_num7':'sum'}).withColumnRenamed(\"sum(etc_num7)\", \"playtime\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature name : sum_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 104).groupBy('account').agg({'etc_num1':'sum'}).withColumnRenamed(\"sum(etc_num1)\", \"sum_money\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_label.count(), len(train_label.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple groupby : https://stackoverflow.com/questions/36251004/pyspark-aggregation-on-multiple-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player action features\n",
    "- Sit (count / ratio / per_day_count)\n",
    "- Exp_get\n",
    "- item_get\n",
    "- money_get\n",
    "- abyss_get\n",
    "- Exp_repair (count / per_day_count)\n",
    "- Use_portal\n",
    "- Killed_bypc\n",
    "- Killed_bynpc\n",
    "- Teleport\n",
    "- Reborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_features(train, test, df, name, count, ratio, per_day_count, log_id, etc_num):\n",
    "    if per_day_count == True:\n",
    "        print(\"Processing per_day_count features...\")\n",
    "        df_agg = df.filter(df['log_id'] == log_id).groupBy(['account', 'date']).agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"{}_per_day_count\".format(name))\n",
    "        df_agg = df_agg.groupby(['account']).agg({\"{}_per_day_count\".format(name):'mean'}).withColumnRenamed(\"mean({}_per_day_count)\".format(name), \"{}_per_day_count\".format(name))\n",
    "        train = train.join(df_agg, ['account'], how='left')\n",
    "        test = test.join(df_agg, ['account'], how='left')\n",
    "    if count == True:\n",
    "        print(\"Processing count features...\")\n",
    "        df_agg = df.filter(df['log_id'] == log_id).groupBy('account').agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"{}_count\".format(name))\n",
    "        train = train.join(df_agg, ['account'], how='left')\n",
    "        test = test.join(df_agg, ['account'], how='left')\n",
    "        #if ratio == True:\n",
    "        #    # \"{}_count\".format(name)\n",
    "        #    # Count (총) / Sum (총)\n",
    "        #    # etc_num을 Sum으로 나누면 Ratio가 됨. \n",
    "        #    df_agg = df.filter(df['log_id'] == log_id).groupBy('account').agg({'account_account':'count', '{}'.format(etc_num):'sum'}).withColumnRenamed(\"count(account_account)\", \"{}_count\".format(name)).withColumnRenamed(\"sum({})\".format(etc_num), \"{}_sum\".format(name))\n",
    "        #    df = df.join(df_agg, ['account'], how='left')\n",
    "        #    df = df.withColumn('{}_ratio'.format(name), df['etc_num']/df[\"{}_sum\".format(name)]).drop(\"{}_sum\".format(name))\n",
    "    if ratio == True:\n",
    "        print(\"Processing ratio features...\")\n",
    "    else:\n",
    "        pass\n",
    "    print(\"Processing end\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sit information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'sit', log_id = 118 , etc_num = 'actor_account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# Exp_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Exp_get', log_id = 143 , etc_num = 'actor_account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# item_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'item_get', log_id = 225 , etc_num = 'actor_account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# money_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'money_get', log_id = 187 , etc_num = 'actor_account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# abyss_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'abyss_get', log_id = 156 , etc_num = 'actor_account', count = True, ratio = True, per_day_count = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp_repair information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Exp_repair', log_id = 148 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Use_portal False\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Use_portal', log_id = 151 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Killed_bypc information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Killed_bypc', log_id = 137 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Killed_bynpc information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Killed_bynpc', log_id = 138 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Teleport information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Teleport', log_id = 142 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Reborn information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total_sample, name = 'Reborn', log_id = 145 , etc_num = 'actor_account', count = True, ratio = False, per_day_count = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_label.count(), len(train_label.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group activities features\n",
    "- Avg_PartyTime\n",
    "- GuildAct_count (계산 못하겠음)\n",
    "- GuildJoin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg_PartyTime\n",
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 127).groupBy('account').agg({'etc_num7':'mean'}).withColumnRenamed(\"mean(etc_num12)\", \"Avg_PartyTime\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GuildJoin_count\n",
    "df_total_sample_agg = df_total_sample.filter(df_total_sample['log_id'] == 605).groupBy('account').agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"GuildJoin_count\")\n",
    "train_label = train_label.join(df_total_sample_agg, ['account'], how='left')\n",
    "test_label = test_label.join(df_total_sample_agg, ['account'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network measures features\n",
    "- Party \n",
    "- Friend\n",
    "- Trade \n",
    "- mail \n",
    "- dual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting \n",
    "방법1. \n",
    "- 패키지 설치 : pip install graphframes\n",
    "- 패키지 로드 : from graphframes import *\n",
    "- graphframes example : https://towardsdatascience.com/graphframes-in-jupyter-a-practical-guide-9b3b346cebc5\n",
    "\n",
    "방법2. \n",
    "하드코딩 \n",
    "- In_degree : 나한테 들어오는 갯수 \n",
    "- out_degree : 나에서 나가는 갯수 \n",
    "- Eccentricity : The eccentricity of a node s is the longest shortest path d between this node and all other nodes t of the network:\n",
    "\n",
    "방법3. \n",
    "- python : https://www.kirenz.com/post/2019-08-13-network_analysis/\n",
    "- 이론 : http://blog.naver.com/PostView.nhn?blogId=happyrachy&logNo=221273644056&parentCategoryNo=&categoryNo=1&viewDate=&isShowPopularPosts=true&from=search\n",
    "- 이론2 : https://bab2min.tistory.com/554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use graphframes in Jupyter notebook by referencing graphrames.jar\n",
    "- https://github.com/graphframes/graphframes/issues/104#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_network_features(train, test, df, log_id, name):\n",
    "    df_agg1 = df.filter(df['log_id'] == log_id).groupBy(['account']).agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"{}_in_deg\".format(name))\n",
    "    df_agg2 = df.filter(df['log_id'] == log_id).groupBy(['target_account']).agg({'target_account':'count'}).withColumnRenamed(\"count(target_account)\", \"{}_out_deg\".format(name))\n",
    "\n",
    "    train = train.join(df_agg1, ['account'], how='left')\n",
    "    test = test.join(df_agg1, ['account'], how='left')\n",
    "\n",
    "    train = train.join(df_agg2, ['account'], how='left')\n",
    "    test = test.join(df_agg2, ['account'], how='left')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# party \n",
    "train_label, test_label = social_network_features(train_label, test_label, df_total_sample_agg, log_id = 126, name = 'p')\n",
    "\n",
    "# Friend \n",
    "train_label, test_label = social_network_features(train_label, test_label, df_total_sample_agg, log_id = 134, name = 'f')\n",
    "\n",
    "# Dual\n",
    "train_label, test_label = social_network_features(train_label, test_label, df_total_sample_agg, log_id = 158, name = 'd')\n",
    "\n",
    "# Mail \n",
    "train_label, test_label = social_network_features(train_label, test_label, df_total_sample_agg, log_id = 229, name = 'm')\n",
    "\n",
    "# Trade \n",
    "train_label, test_label = social_network_features(train_label, test_label, df_total_sample_agg, log_id = 210, name = 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.write.csv(\"train_label.csv\")\n",
    "test_label.write.csv(\"test_label.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
