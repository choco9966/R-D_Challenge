{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm_notebook # check the progressbar in the python. \n",
    "import glob # check the file name in fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-2.4.4-bin-hadoop2.7\\\\spark-2.4.4-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "path = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final\\\\fin_test_accounts.csv',\n",
       " 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final\\\\labeled_accounts.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file_lst = glob.glob(path + '*')\n",
    "read_file_lst[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff07f0063ee40618ccd15811af5a4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`account`' given input columns: [etc_num14, worldnum, actor_account, etc_num15, etc_num11, etc_num1, item_uid, Date, etc_num4, etc_num2, etc_str1, actor, etc_num10, etc_num9, etc_num7, log_id, etc_str3, etc_num3, target_account, etc_num6, etc_num5, etc_num8, big_log_id, etc_num12, log_date, target, etc_str2, etc_num13];;\\n'Project ['account, Date#101]\\n+- Project [log_date#47, big_log_id#48, log_id#49, actor#50, actor_account#51, target#52, target_account#53, worldnum#54L, etc_str1#55, etc_str2#56, etc_str3#57, etc_num1#58L, etc_num2#59L, etc_num3#60L, etc_num4#61, etc_num5#62L, etc_num6#63L, etc_num7#64L, etc_num8#65L, etc_num9#66L, etc_num10#67, etc_num11#68L, etc_num12#69L, etc_num13#70L, ... 4 more fields]\\n   +- Relation[log_date#47,big_log_id#48,log_id#49,actor#50,actor_account#51,target#52,target_account#53,worldnum#54L,etc_str1#55,etc_str2#56,etc_str3#57,etc_num1#58L,etc_num2#59L,etc_num3#60L,etc_num4#61,etc_num5#62L,etc_num6#63L,etc_num7#64L,etc_num8#65L,etc_num9#66L,etc_num10#67,etc_num11#68L,etc_num12#69L,etc_num13#70L,... 3 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`account`' given input columns: [etc_num14, worldnum, actor_account, etc_num15, etc_num11, etc_num1, item_uid, Date, etc_num4, etc_num2, etc_str1, actor, etc_num10, etc_num9, etc_num7, log_id, etc_str3, etc_num3, target_account, etc_num6, etc_num5, etc_num8, big_log_id, etc_num12, log_date, target, etc_str2, etc_num13];;\n'Project ['account, Date#101]\n+- Project [log_date#47, big_log_id#48, log_id#49, actor#50, actor_account#51, target#52, target_account#53, worldnum#54L, etc_str1#55, etc_str2#56, etc_str3#57, etc_num1#58L, etc_num2#59L, etc_num3#60L, etc_num4#61, etc_num5#62L, etc_num6#63L, etc_num7#64L, etc_num8#65L, etc_num9#66L, etc_num10#67, etc_num11#68L, etc_num12#69L, etc_num13#70L, ... 4 more fields]\n   +- Relation[log_date#47,big_log_id#48,log_id#49,actor#50,actor_account#51,target#52,target_account#53,worldnum#54L,etc_str1#55,etc_str2#56,etc_str3#57,etc_num1#58L,etc_num2#59L,etc_num3#60L,etc_num4#61,etc_num5#62L,etc_num6#63L,etc_num7#64L,etc_num8#65L,etc_num9#66L,etc_num10#67,etc_num11#68L,etc_num12#69L,etc_num13#70L,... 3 more fields] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-518ca837f1bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdf_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munionAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# row-wise 결합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-239710aa38d8>\u001b[0m in \u001b[0;36munionAll\u001b[1;34m(dfs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munionAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-239710aa38d8>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(df1, df2)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munionAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`account`' given input columns: [etc_num14, worldnum, actor_account, etc_num15, etc_num11, etc_num1, item_uid, Date, etc_num4, etc_num2, etc_str1, actor, etc_num10, etc_num9, etc_num7, log_id, etc_str3, etc_num3, target_account, etc_num6, etc_num5, etc_num8, big_log_id, etc_num12, log_date, target, etc_str2, etc_num13];;\\n'Project ['account, Date#101]\\n+- Project [log_date#47, big_log_id#48, log_id#49, actor#50, actor_account#51, target#52, target_account#53, worldnum#54L, etc_str1#55, etc_str2#56, etc_str3#57, etc_num1#58L, etc_num2#59L, etc_num3#60L, etc_num4#61, etc_num5#62L, etc_num6#63L, etc_num7#64L, etc_num8#65L, etc_num9#66L, etc_num10#67, etc_num11#68L, etc_num12#69L, etc_num13#70L, ... 4 more fields]\\n   +- Relation[log_date#47,big_log_id#48,log_id#49,actor#50,actor_account#51,target#52,target_account#53,worldnum#54L,etc_str1#55,etc_str2#56,etc_str3#57,etc_num1#58L,etc_num2#59L,etc_num3#60L,etc_num4#61,etc_num5#62L,etc_num6#63L,etc_num7#64L,etc_num8#65L,etc_num9#66L,etc_num10#67,etc_num11#68L,etc_num12#69L,etc_num13#70L,... 3 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "for i in tqdm_notebook(read_file_lst):\n",
    "    # sqlContext.read.csv() : path를 C:Users/..의 형식으로 맞춰줘야함. (초기값이 spark설치된 곳으로 지정되어있음)\n",
    "    df_temp = sqlContext.read.csv(SparkFiles.get(i), header=True, inferSchema= True) # read.csv \n",
    "    \n",
    "    # transformation으로 추가하려면, df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "    df_temp = df_temp.withColumn(\"Date\", lit(i[-8:-4])) # column 추가, lit명령어가 value를 추가하는 방법. \n",
    "    \n",
    "    if i == read_file_lst[0]:\n",
    "        df_total = df_temp\n",
    "    else:\n",
    "        df_total = unionAll([df_total, df_temp]) # row-wise 결합\n",
    "        \n",
    "    del df_temp \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# userItem=df.groupby('userId').agg(f.expr('count(distinct item)').alias('n_item'))\n",
    "# df_total.groupBy(\"log_id\").agg(f.expr('count(distinct item)').alias('log_id'))\n",
    "# df_total.select(\"actor_account\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group activities features\n",
    "- Avg_PartyTime\n",
    "- GuildAct_count (계산 못하겠음)\n",
    "- GuildJoin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.withColumnRenamed(\"actor_account\", \"account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg_PartyTime\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 127).select('account','etc_num12').toPandas()\n",
    "df_total_agg = df_total_agg.groupby(['account'])['etc_num12'].agg({'sum','mean'}).reset_index().rename(columns={'sum':'Sum_PartyTime', 'mean':'Avg_PartyTime'})\n",
    "train_label = train_label.merge(df_total_agg, on = 'account', how='left')\n",
    "test_label = test_label.merge(df_total_agg, on = 'account', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GuildJoin_count\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 605).select('account').toPandas()\n",
    "df_total_agg = df_total_agg.groupby(['account'])['account'].agg({'count'}).reset_index().rename(columns={'count':'GuildJoin_count'})\n",
    "train_label = train_label.merge(df_total_agg, on = 'account', how='left')\n",
    "test_label = test_label.merge(df_total_agg, on = 'account', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_group.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_group.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 103).select('account').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['account'].agg({'count'}).reset_index().rename(columns={'count':'login_count'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 103).select('account','date').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['date'].agg({'nunique'}).reset_index().rename(columns={'nunique':'Day_unique_count'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 103).select('account','etc_str1').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['etc_str1'].agg({'nunique'}).reset_index().rename(columns={'nunique':'ip_nunique'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 103).select('account','etc_num2').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['etc_num2'].agg({'max'}).reset_index().rename(columns={'max':'max_level'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 104).select('account','etc_num7').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['etc_num7'].agg({'mean', 'sum'}).reset_index().rename(columns={'mean':'mean_playtime', 'sum':'sum_playtime'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 104).select('account','etc_num1').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['etc_num1'].agg({'mean', 'sum'}).reset_index().rename(columns={'sum':'sum_money'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_agg = df_total.filter(df_total['log_id'] == 116).select('account','etc_num7','etc_num8','etc_num9','etc_num10').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')[['etc_num7','etc_num8','etc_num9','etc_num10']].agg({'mean'}).reset_index()\n",
    "df_total_agg.columns = ['account', 'captcha_count_mean', 'captcha_ban_time_mean', 'captcha_prior_time_mean', 'captcha_occur_time_mean']\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_player.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_player.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_network_features(train, test, df, log_id, name):\n",
    "    # In-degree\n",
    "    df_agg1 = df.filter(df['log_id'] == log_id).groupBy(['account']).agg({'account':'count'}).withColumnRenamed(\"count(account)\", \"{}_in_deg\".format(name))\n",
    "    # Out-degree\n",
    "    df_agg2 = df.filter(df['log_id'] == log_id).groupBy(['target_account']).agg({'target_account':'count'}).withColumnRenamed(\"count(target_account)\", \"{}_out_deg\".format(name))\n",
    "    df_agg2 = df_agg2.withColumnRenamed(\"target_account\", \"account\")\n",
    "    \n",
    "    df_agg1 = df_agg1.select('account',\"{}_in_deg\".format(name)).toPandas()\n",
    "    df_agg2 = df_agg2.select('account',\"{}_out_deg\".format(name)).toPandas()\n",
    "    # Python code로 짜야하는데 어려움 ㅜㅜ... \n",
    "    # package는 apache기준. pyspark의 package는 pagerank랑 Component_ID만 있음. \n",
    "    # cc, between, closeness, Eigenvector, Eccentricity, Authority, Hub, Pagerank \n",
    "    \n",
    "    #train = train.join(df_agg1, ['account'], how='left')\n",
    "    test = pd.merge(test, df_agg1, on = 'account', how='left')\n",
    "\n",
    "    #train = train.join(df_agg2, ['account'], how='left')\n",
    "    test = pd.merge(test, df_agg2, on = 'account', how='left')\n",
    "    print(\"end..\")\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# party \n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 126, name = 'p')\n",
    "\n",
    "# Friend \n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 134, name = 'f')\n",
    "\n",
    "# Dual\n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 158, name = 'd')\n",
    "\n",
    "# Mail \n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 229, name = 'm')\n",
    "\n",
    "# Trade1\n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 210, name = 't1')\n",
    "\n",
    "# Trade2\n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 219, name = 't2')\n",
    "\n",
    "# Private Shop\n",
    "test_label = social_network_features(train_label, test_label, df_total, log_id = 247, name = 'p_s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_social.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_social.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_features(train, test, df, name, count, ratio, per_day_count, log_id, etc_num):\n",
    "    df_agg = df.filter(df['log_id'] == log_id).select('account', 'date').toPandas()\n",
    "    df_agg = df_agg.groupby(['account', 'date'])['account'].agg({'count'}).reset_index()\n",
    "    df_agg = df_agg.groupby(['account'])['count'].agg({'mean', 'sum', 'std'}).reset_index().rename(columns={'mean':'{}_per_day_count'.format(name), 'sum':'{}_sum_count'.format(name), 'std':'{}_std_per_day_count'.format(name)})\n",
    "    # train = train.merge(df_agg, on='account' , how='left')\n",
    "    test = test.merge(df_agg, on='account' , how='left')\n",
    "        \n",
    "    df_agg = df.filter(df['log_id'] == log_id).select('account').toPandas()\n",
    "    df_agg = df_agg.groupby(['account'])['account'].agg({'count'}).reset_index().rename(columns={'count':\"{}_count\".format(name)})\n",
    "    # train = train.merge(df_agg, on='account' , how='left')\n",
    "    test = test.merge(df_agg, on='account' , how='left')\n",
    "\n",
    "    print(\"Processing end...\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp_repair information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Exp_repair', log_id = 148 , etc_num = 'account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Use_portal False\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Use_portal', log_id = 151 , etc_num = 'account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Killed_bypc information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Killed_bypc', log_id = 137 , etc_num = 'account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Killed_bynpc information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Killed_bynpc', log_id = 138 , etc_num = 'account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Teleport information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Teleport', log_id = 142 , etc_num = 'account', count = True, ratio = False, per_day_count = True )\n",
    "\n",
    "# Reborn information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'Reborn', log_id = 145 , etc_num = 'account', count = True, ratio = False, per_day_count = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_action2.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_action2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_features(train, test, df, name, count, ratio, per_day_count, log_id, etc_num):\n",
    "    if per_day_count == True:\n",
    "        #print(\"Processing per_day_count features...\")\n",
    "        df_agg = df.filter(df['log_id'] == log_id).select('account', 'date').toPandas()\n",
    "        df_agg = df_agg.groupby(['account', 'date'])['account'].agg({'count'}).reset_index()\n",
    "        df_agg = df_agg.groupby(['account'])['count'].agg({'mean', 'sum'}).reset_index()\n",
    "        df_agg = df_agg.rename(columns={'mean':'{}_per_day_count'.format(name), 'sum':'{}_sum_count'.format(name), 'std':'{}_std_per_day_count'.format(name)})\n",
    "        # train = train.merge(df_agg, on='account' , how='left')\n",
    "        test = test.merge(df_agg, on='account' , how='left')\n",
    "        \n",
    "    if count == True:\n",
    "        #print(\"Processing count features...\")\n",
    "        df_agg = df.filter(df['log_id'] == log_id).select('account').toPandas()\n",
    "        df_agg = df_agg.groupby(['account'])['account'].agg({'count'}).reset_index().rename(columns={'count':\"{}_count\".format(name)})\n",
    "        # train = train.merge(df_agg, on='account' , how='left')\n",
    "        test = test.merge(df_agg, on='account' , how='left')\n",
    "        #if ratio == True:\n",
    "        #    # \"{}_count\".format(name)\n",
    "        #    # Count (총) / Sum (총)\n",
    "        #    # etc_num을 Sum으로 나누면 Ratio가 됨. \n",
    "        #    df_agg = df.filter(df['log_id'] == log_id).groupBy('account').agg({'account_account':'count', '{}'.format(etc_num):'sum'}).withColumnRenamed(\"count(account_account)\", \"{}_count\".format(name)).withColumnRenamed(\"sum({})\".format(etc_num), \"{}_sum\".format(name))\n",
    "        #    df = df.join(df_agg, ['account'], how='left')\n",
    "        #    df = df.withColumn('{}_ratio'.format(name), df['etc_num']/df[\"{}_sum\".format(name)]).drop(\"{}_sum\".format(name))\n",
    "    print(\"Processing end...\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature name : donepoll\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 152).select('account').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['account'].agg({'count'}).reset_index().rename(columns={'count':'count_donepoll'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest_Try\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 501).select('account').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['account'].agg({'count'}).reset_index().rename(columns={'count':'quest_try_count'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest_Success\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 503).select('account').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['account'].agg({'count'}).reset_index().rename(columns={'count':'quest_success_count'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GatherLimitUp\n",
    "df_total_agg = df_total.filter(df_total['log_id'] == 416).select('account','etc_num8').toPandas()\n",
    "df_total_agg = df_total_agg.groupby('account')['etc_num8'].agg({'count', 'sum'}).reset_index().rename(columns={'sum':'sum_gather', 'count':'count_gather'})\n",
    "\n",
    "train_label = train_label.merge(df_total_agg, on='account' , how='left')\n",
    "test_label = test_label.merge(df_total_agg, on='account' , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_action3.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_action3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/dataset/Challenge19_GameBot_Final/'\n",
    "train_label = pd.read_csv(i + \"labeled_accounts.csv\")\n",
    "test_label = pd.read_csv(i + \"fin_test_accounts.csv\")\n",
    "del train_label['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_features(train, test, df, name, count, ratio, per_day_count, log_id, etc_num):\n",
    "    df_agg = df.filter(df['log_id'] == log_id).select('account', 'date').toPandas()\n",
    "    df_agg = df_agg.groupby(['account', 'date'])['account'].agg({'count'}).reset_index()\n",
    "    df_agg = df_agg.groupby(['account'])['count'].agg({'mean', 'sum'}).reset_index()\n",
    "    df_agg = df_agg.rename(columns={'mean':'{}_per_day_count'.format(name), 'sum':'{}_sum_count'.format(name), 'std':'{}_std_per_day_count'.format(name)})\n",
    "    # train = train.merge(df_agg, on='account' , how='left')\n",
    "    test = test.merge(df_agg, on='account' , how='left')\n",
    "        \n",
    "    df_agg = df.filter(df['log_id'] == log_id).select('account').toPandas()\n",
    "    df_agg = df_agg.groupby(['account'])['account'].agg({'count'}).reset_index().rename(columns={'count':\"{}_count\".format(name)})\n",
    "    # train = train.merge(df_agg, on='account' , how='left')\n",
    "    test = test.merge(df_agg, on='account' , how='left')\n",
    "\n",
    "    print(\"Processing end...\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sit information\n",
    "# train_label, test_label = action_features(train_label, test_label, df_total, name = 'sit', log_id = 118 , etc_num = 'account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# Exp_get information\n",
    "# train_label, test_label = action_features(train_label, test_label, df_total, name = 'Exp_get', log_id = 143 , etc_num = 'account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# item_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'item_get', log_id = 225 , etc_num = 'account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# money_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'money_get', log_id = 187 , etc_num = 'account', count = True, ratio = True, per_day_count = True )\n",
    "\n",
    "# abyss_get information\n",
    "train_label, test_label = action_features(train_label, test_label, df_total, name = 'abyss_get', log_id = 156 , etc_num = 'account', count = True, ratio = True, per_day_count = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\User\\Documents\\R&D Challenge2019\n",
    "outputpath = 'C:/Users/User/Documents/R_D Challenge2019/Challenge19_GameBot_Preliminary/features2/'\n",
    "train_label.to_csv(outputpath + \"train_action1.csv\", index=False)\n",
    "test_label.to_csv(outputpath + \"test_action1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
